---
title: "Cluster API WG"
date: 2018-07-19
tags: ["Cluster API", "SIG Cluster Lifecycle"]
draft: true
---

# Cluster API WG

* [Azure Provider developed at Platform9](https://github.com/platform9/azure-provider)
  * Is anyone interested in adopting it as an official provider for Azure
  * Been working on it for the last month, but it's not mature yet
  * Open a tracking issue and make sure it meets the bar for a SIG-sponsored repo
* Transfer of ownership to k8s-sigs for [cluster-api-provider-aws](https://github.com/detiber/cluster-api-provider-aws)
  * Skeleton impl of AWS provider for cluster-api
  * Gain agreement on initial design spec among stakeholders prior to impl
* [cluster-api-provider-openstack](https://github.com/kubernetes-sigs/cluster-api-provider-openstack) has been created
* Should we plan on having multiple impls for each provider in the same repo?
  * Yes, as much as possible. Allows us to share common code even if there are
    multiple impls
* Transfer of ownership to k8s-sigs for [cluster-api-provider-gcp](https://github.com/roberthbailey/cluster-api-provider-gcp)
  * Plan to delete the `cloud/google` dir from the cluster-api repo once it's in
    place
* [To ssh](https://github.com/kubernetes-sigs/cluster-api/issues/160) or [not to ssh](https://github.com/kubernetes-sigs/cluster-api/issues/122)
  * Bootstrapping the cluster is difficult
  * Trying to configure the chain and grab the serving cert
  * Had talked about a protocol (over `grpc`) to allow machine to act as a server
  * Problem with `ssh` is that you need network connectivity, problem behind
    NAT, bastion, etc.
  * Harder problem when in a private network
  * Get input to the machine but not output
  * Pre-create CA and certs and pass them in
  * `cloud-init` works for VMs but not bare metal
  * `cloud-init` doesn't have a good way to report status about what failed
  * [Kops received a `NodeAuthorizationService`](https://github.com/kubernetes/kops/pull/5317) that might be useful
    * Trying to find a more secure alternative to kubelet bootstrapping that
      used a server & client component
  * Right now you can generate a bootstrap token for each machine and an IP and
    machines can bootstrap themselves
  * Kops PR is doing it with a handshake where the machine controller is creating
    the token and injecting it into the machine
  * `clusterctl` is trying to apply addons and is staging the machines to deploy
    control plane machine before worker nodes. Still need a kubeconfig to do that
  * `kops` pre-seeds the information, somewhat by mistake - stores data in S3
    and the master pulls it.
  * For HA is there an alternative to creating the certs/keys at a higher level?
  * If you can access the cluster with sufficient permissions you can run commands
    on the node through pods
  * Don't have to pre-seed, but whether you generate the cert/key on the master
    or externally, it has to be pulled out and transferred. If orchestration needs
    a copy then maybe it can be used for the kubeconfig problem now.
* [Alpha exit criteria](https://github.com/kubernetes-sigs/cluster-api/issues/250)
  * Still talking about `apiserverbuilder`?
* Two RedHat employees introducing and offering to help on `cluster-api`
* [Packet Provider](https://github.com/kubernetes-sigs/cluster-api/issues/431) for CNCF and cross-cloud CI
* [Provider Implementers Office Hours vote](https://doodle.com/poll/3ieeufbfz522x8yd)

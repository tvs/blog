---
title: "May 23, 2018"
date: 2018-05-23
tags: ["cfcr"]
draft: false
---
# May 23, 2018

## CFCR
  * Architecture lags beind the Kubernetes features that are offered in GKE, Kubeadm and others
    - [Self-hosted](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cluster-lifecycle/self-hosted-kubernetes.md#what-is-self-hosted)
      * [Self-Hosting the Master Control Plane](https://docs.google.com/document/d/199sctQefgQ9pPCP3jRB3oXgw7IaGBaYE9wrE05-j4E4/view)
    - [TLS Bootstrapping](https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/)
      * [Kubo-Release PR](https://github.com/cloudfoundry-incubator/kubo-release/pull/179)
      * [Kubo-Deployment PR](https://github.com/cloudfoundry-incubator/kubo-deployment/pull/269)
    - [Live Kubelet Reconfiguration](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/)
    - [Kubernetes Autoscaler](https://github.com/kubernetes/autoscaler)
      * Horizontal Pod Autoscaler
      * Vertical Pod Autoscaler
      * Cluster Autoscaler
    - [Cloud Controller Manager](https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/)
    - [Cluster-Level Logging Architectures](https://kubernetes.io/docs/concepts/cluster-administration/logging#cluster-level-logging-architectures)
    - [Certificate Signing](https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/)
      * This has been solved recently and so cluster-administrators can now create and empower
        additional users on the cluster.
    - [Out-of-Resource Handling](https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource) for default components
    - [Extensible Admission Control](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/)

  * Too rigid to act as a foundation for other platforms (e.g. PKS)
    - No pluggable CNI, operators are locked in to Flannel (which lacks a great many features)
      * At the very least, we could consider offering [Canal](https://github.com/projectcalico/canal), which offers many of the missing
        network policy features.
    - No control over many Kubernetes-offered features
      * Admission Plugins: [Kubo-Release PR](https://github.com/cloudfoundry-incubator/kubo-release/pull/206), [Kubo-Deployment PR](https://github.com/cloudfoundry-incubator/kubo-deployment/pull/310)
        * Allows platforms to add custom webhook admission plugins to better control intended
          tenancy relationships
    - Cloud Config is constrained to the properties defined in the job
      * The structure of cloud-config.ini is poorly documented, varies from patch to patch
        and from provider to provider, and will be impossible to track fully if CFCR supports
        out-of-tree cloud providers.
      * The existing config is out of date and lacks many properties for the different
        cloud providers.
    - No ability to pick between stable and edge Kubernetes (e.g. 1.9.x and 1.10.x)
    - CFCR testing relies entirely on the _way_ in which the cluster is deployed, meaning that
      other platforms and consumers are unable to verify that their deployment operates as
      expected.
    - Certificate rotation problems as clusters begin to near 1yr old
      * Credhub's certificate rotation mechanism doesn't lend itself well to CFCR, where
        the cluster administrator can utilize the CA to sign certificates through Kubernetes
      * When the CA expires it'll be necessary to regenerate via Credhub, which doesn't appear to
        preserve the private key, thereby invalidating all previously-issued certificates.

  * Support contracts are implicit rather than explicit
    - Lack direct testing for scenarios and components that downstream products rely on.
      Many of these scenarios and components require support at the CFCR-level and can't
      simply be strapped on.
      * NSX-T
      * Airgapped Environments
    - CFCR Forks
      * CFCR does not offer any support for older releases, which means that products built on
        those are left to their own devices. This has lead to PKS mantaining their own fork that
        is regularly missing fixes or improvements that ought to be pushed downstream. Conversely,
        fixes that PKS has made do not find their way upstream.
      * Since CFCR is part and parcel of PKS, it would make sense to offer support contracts for
        previous versions.
      * PKS can still take primary stewardship of those branches as all of the closed-source juju
        can (and does) exist in separate projects, but CFCR can more readily communicate cherry-picks
        both into and out of the branch.
    - Master branch is a working branch, which makes it difficult for downstream products to know
      what is and isn't stable.
      * As a result, the only mechanism they have for canary testing is to take whatever comes off
        the end of the pipeline, which given pipeline instability, often jumps from a week(s)-old
        commit to something many commits later, making it difficult to identify any underlying problem.
      * Features should be developed in branches and merged into the master once testing has completed.
        * Github PRs would better communicate CFCR's work to the wider community rather than the
          relatively opaque Tracker.

  * Reliability, scalability, security, and disaster testing are ignored or done downstream
    - Release Engineering is focused largely on PKS, which means that CFCR doesn't get
      the "enterprise readiness" check on its own, it occurs only as a side-effect
      of PKS testing
      * It's difficult to build an "enterprise ready" product atop a shaky foundation.
    - Disaster Testing/Chaos Testing doesn't seem to be occurring at the cluster level
      * Turbulence tests only test against BOSH recovery - ensuring that workers and masters
        can rejoin the cluster.
      * Chaos testing of Kubernetes can and will surface issues with default workloads, scalability
        and recovery.
        * Example: when a node runs out of resources it begins to evict pods, some of which are key to
          the cluster's operation (e.g. `kube-dns`)
    - Security testing comes from the interaction with PKS, which is difficult to separate from core
      concerns with CFCR.
      * As an example, PKS's Cody shouldn't be considered the same person as CFCR's Bonnie, yet they
        both use the same credentials. The ability to mount privileged containers and snoop around VMs
        is a security concern only for PKS but is having additional implications that fall through to
        CFCR.
      * It's also slow to propagate those security concerns to the rest of the team because they're
        often kept "need to know" and have to filter through the PMs of each team before they reach
        engineers.

  * Team lacks Kubernetes knowledge
    - Across both CFCR and PKS, the engineering and product management teams seem to lack knowledge
      of Kubernetes. New features go unnoticed, configuration options ignored, and expected behaviors
      are unclear.
    - The product gets led around by nose when customers decide they want something rather than product
      and engineering being able to anticipate their needs and enable solutions.
    - High rate of turnover in team composition both from engineering and product stifles the ability to
      gain deeper understandings of Kubernetes. Just as members are coming to understand Kubernetes they
      get rotated out and their replacement starts from scratch.
      * What happens once Dublin runs out of Pivots to rotate through the team? Do old faces start to
        rotate back in? Does the product move locations?

## Interteam Relationships

  * Teams are distributed but compartmentalized
    - CFCR, PKS, Release Engineering and the Integrations teams are distributed all over the globe, but
      the teams are compartmentalized into regions.
      * For example, CFCR and PKS are largely only located in Dublin. Because of this, when related teams
        have issues, there's anywhere from 8 to 16 hours of delay between an issue being spotted in PST
        and a response in IST.
      * Teams could be better distributed across the time zones, which would facilitate a greater deal of
        communication and collaboration.
        * As an example, Alton and I have paired extensively over the past few weeks to work on enabling
          multi-master in PKS and responding to bugs that crop up in Releng and Integrations.

---
title: "TLS Bootstrapping Rundown"
date: 2018-06-14
tags: []
draft: true
---

# Bootstrapping

## Flow Diagram

![Bootstrapping Flow](/img/Bootstrapping Flow.png)

## Testing

* "Interactions between the internal components were untested"
  * They were not tested by directly looking at communication traffic, but they
    are tested through various other features.
      * Heapster, for example, talks directly to the Kubelet through TLS using
        its service account credentials.  Heapster, thereby, validates that the
        Kubelet's server certificate is accurate and the Kubelet verifies that
        the incoming request is using a proper client certificate.
        Additionally, the Kubelet verifies the service account token through
        the webhook authorization mechanism.  If certificates were
        misconfigured, webhook was disabled, or RBAC permissions missing,
        Heapster would be unable to scrape any data. In turn, that means that
        the `kubectl top` tests in the integration suite would also fail.
  * There are no explicit tests to verify that other components are speaking over
    TLS either:
      * Flannel -> etcd
      * API server -> etcd

* "Features enabled by TLS Bootstrapping were also untested (_NodeRestriction_)"
  * We trust most of our Kubernetes flags are in fact tested and operating
    correctly simply based on our enabling of the flag.
  * Controllers: `LimitRanger`, `NamespaceLifecycle`, `ResourceQuota`, `ServiceAccount`
    `MutatingAdmissionWebhook`, `DenyEscalatingExec`, `SecurityContextDeny`, etc.
  * Flags: `--enable-swagger-ui`, `--repair-malformed-updates=false`,
    aggregator-oriented flags, etc.
  * Most of our testing is around features and components that can be affected
    by the various other features we deploy: the cloud config, `kube-proxy`, etc.
  * `NodeRestriction` _is_ tested. Every single time we deploy a cluster. If
    the certificates were not being generated or invalid, then the `NodeRestriction`
    controller would prevent the cluster from even coming together. That's at
    the very core of the controller.

* "Not enough validation that the requested certificate was actually from the worker vm"
  * Needing iteration on that validation isn't justification to roll back
    everything entirely. The _worst_ that happens is that a certificate gets
    generated for a malicious Kubelet and accidentally approved.
      * Building a malicious Kubelet is something that can happen today via
        `hostPath` mounts of the Kubelet creds and the Docker socket.
  * There _are_ improvements that we can make. Some are trivial validations
    that can be added to the existing solution. Some require the creation of
    a brand new approval method.
      * Building a brand new approval method is _hard_. It's something that
        Kubernetes has been working on since 1.6 (earlier?) and has struggled
        with. There may be additional information we can get from BOSH and leverage
        for our own narrow purposes, but it's difficult to discover.
      * In the meanwhile, the existing solution is _no less secure_ than what
        we've been shipping with, and covers up our other, more drastic
        security flaws.

* "Don't understand"
  * The majority of the PR stems directly from the [Kubernetes Doc Site](https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/)
  * Other changes to RBAC roles, Heapster, etc. stem from having a secured Kubelet
    and are necessary to keep those components running.
  * The only piece of custom code tacked on is the auto-approval.
  * Any new bootstrapping-based solution we build will not be fundamentally
    different. The only question is of the approver.

## Difficulties in Using a Controller or Daemon

* The controller has to inspect _all_ inbound CSRs, including ones generated by
  arbitrary usersi.
  * It must validate _everything_:
    * `CN`: Must match the node-name, a property determined by the cloud provider
      and must be a member of the group `system:nodes`
    * `Subject Alternative Name`:
      * `IP Address`: `InternalIP`, `ExternalIP`, `CNI Network Address`,
        `Private IP Address`, more?
      * `DNS`: GCP has two identical, don't know how they're supposed to differ
  * How do you validate the DNS names?

* How do we know when the CSR has been approved?
  * When do we uncordon the node?

* The bootstrap token should only live for the duration of bootstrapping
  * How do we maintain that tight locality without generating it as part of
    the Kubelet startup?
  * Creating the Bootstrap token as a user _without using the token auth file_
    requires approving a short-lived certificate and attaching an RBAC role
  * The Kubelet should be able to re-bootstrap itself in the event of lost
    connectivity at the time of a certificate expiration.
    * Kubelet begins to try to rotate certificates around halfway through expiry
      so there's a lot of time for it to regain connectivity to the API, but
      it's possible for it to remain disconnected. Especially if an operator
      configures a shorter expiry.
    * There may be no way to do this without long-lived creds sitting on the
      worker short of forcing BOSH to recreate the worker.
  * Creating a delegated "CSR-signer" user is effectively the same as creating
    a `cluster-admin`.

## Security Concerns

* Leaking the `cluster-admin` creds without privileged containers
  * `hostPath` mount the `cluster-admin` kubeconfig.
  * Nodes are ineligible for workload scheduling until after the file has been
    deleted.
  * Anything that would cause a retemplating occurs after the Node has either
    been drained (upgrade) or after the Node has already become inoperable
    (resurrection). At which point, the Node has restarted and is cordoned off.

* Leaking the `cluster-admin` creds _with_ privileged containers
  * Install software directly to the VM that watches for the `cluster-admin`
    kubeconfig to reappear and then promptly mail it off.
  * Privileged containers currently throw all bets off on workload security.
  * No means to differentiate between which users can and can not deploy
    privileged containers.
  * Effectively makes all users `cluster-admin`, so there's no real escalation
    of privilege.
  * This can be addressed with other mechanisms like `PodSecurityPolicy`
    that only allows `cluster-admin` to deploy privileged containers (and
    thus there's no escalation) and every other user is disallowed.
      * [Enable PodSecurityPolicy](https://github.com/cloudfoundry-incubator/kubo-release/pull/216)

* Adding a malicious Kubelet to the cluster *(can already do this today)*
  * `hostPath` mount the bootstrap token and Docker socket
  * run a custom, malicious Kubelet that can snoop into the workloads,
    record inboud `logs` and `proxy` requests (snatching auth tokens that get
    forwarded from real users), attaches extra containers to workloads,
    etc.
  * Deploy resource-hog workloads using node-affinity to the other Kubelets
    so that all workloads wind up on the malicious Kubelet.

